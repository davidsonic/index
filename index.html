<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Duanjiali.GitHub.io : Jiali Duan&#39;s Academic">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/css/bootstrap.min.css">
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.4/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.4.1/js/bootstrap.min.js"></script>
  
    <link href="style.css" rel="stylesheet">
    <link href="extra.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300,500' rel='stylesheet' type='text/css'>
    <link href='stylesheets/github-light.css' rel='stylesheet' type='text/css'>
    <link href='stylesheets/sytlesheet.css' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.13.0/css/all.min.css"/>
    <link href="https://fonts.googleapis.com/css2?family=Baloo+Da+2:wght@400;500;600;700;800&family=Josefin+Slab:ital,wght@0,400;0,600;1,300;1,400;1,600&family=Muli:ital,wght@0,300;0,400;0,500;1,300;1,400;1,500&display=swap" rel="stylesheet"/>
    <title>Jiali Duan's Site</title>
  </head>

<body>

<!-- spinner -->
<!-- <div class="spinner-container">
  <div class="circles">
    <div></div>
    <div></div>
    <div></div>
    <div></div>
    <div></div>
    <div></div>
    <div></div>
    <div></div>
  </div>
</div> -->

<div class="bg-container">
  <div class="hamburger-menu">
    <div class="line line-1"></div>
    <div class="line line-2"></div>
    <div class="line line-3"></div>
    <span>Close</span>
  </div>

  <!-- <header class="bg-header">
    <div class="bg-img-wrapper">
      <img src="bg-ground.jpeg" />
    </div>

    <div class="banner">
      <h2>酒困路长惟欲睡，日高人渴漫思茶。</h2>
      <h3>《浣溪沙·簌簌衣巾落枣花》 - 宋代 - 苏轼</h3>
      <a href="#section-1">Scroll</a>
    </div>
  </header> -->

  <!-- sidebar -->
  <section class="sidebar">
    <ul class="menu">
      <li class="menu-item">
        <a href="#section-1" class="menu-link" data-content="Intro">Intro</a>
      </li>
      <li class="menu-item">
        <a href="#section-4" class="menu-link" data-content="Publications">Publications</a>
      </li>
      <li class="menu-item">
        <a href="https://davidsonic.github.io/academic_blog/" class="menu-link" data-content="Blogs">Blogs</a>
      </li><br>
      <li class="menu-item">
        <a href="https://davidsonic.github.io/" class="menu-link" data-content="Codes">Codes</a>
      </li>
      <!-- <li class="menu-item">
        <a href="https://davidsonic.github.io/Reading_Posts/" class="menu-link" data-content="Readings">Readings</a>
      </li> -->
    </ul>
    <div class="social-media">
      <a href="https://davidsonic.github.io/"><i class="fab fa-github-alt"></i></a>
      <a href="https://davidsonic.github.io/academic_blog/"><i class="fab fa-fly"></i></a>
      <a href="https://www.linkedin.com/in/jiali-duan-bb5917167/"><i class="fab fa-linkedin"></i></a>
    </div>
  </section>
  
</div>


<!-- HEADER -->
<section class="section-1" id="section-1">

  <!-- Navbar -->
  <nav class="navbar center">
    <a href="#section-1" class="navbar-link">Home</a>
    <a href="#section-2" class="navbar-link">About</a>
    <a href="#section-3" class="navbar-link">CV/Projects</a>
    <a href="#section-4" class="navbar-link">Publication</a>
    <a href="#section-5" class="navbar-link">Activity</a>
    <a href="#section-6" class="navbar-link">Contact</a>
  </nav>
  <div class="photo-projects">

    <div id="header" class="bg1">
      <div id="headerblob">
        <img src="images/Jiali-2023-crop.png" class="img-circle imgme" style="height:200px; width=250px;">
        <div id="headertext">
          <div id="htname" style="font-weight:500;font-size:36px" >Jiali Duan (段佳利)</div>
          <div id="htdesc" style="font-weight:500">Researcher @ Apple</div>
          <div id="htem" style="font-weight:500">jli.duan@gmail.com</div>
            <div id="htem">
                <a class="header-link" href="JialiDuan_Academic_Resume_2024.pdf">Resume</a>&nbsp;&nbsp;
                <a class="header-link" href="https://www.linkedin.com/in/jiali-duan-bb5917167/">Linkedin</a>&nbsp;&nbsp;
                <a class="header-link" href="https://scholar.google.com/citations?user=SjcMvxwAAAAJ&hl=en">Google Scholar</a></div>
        </div>
      </div>
    </div>

    <!--
    <div class="projects">
      <div class="cards-wrapper">
        <div class="card" data-tilt>
          <div class="card-img-wrapper">
            <img src="images/openai-clip.png" alt="img" />
          </div>
          <div class="card-info">
            <h3>Multimodal Learning</h3>
            <p>
              "CLIP, DALLE, VLP;"
            </p>
            <button><a href="SLADE/CUB200-demo.html">Read More</a></button>
          </div>
        </div>
        <div class="card" data-tilt>
          <div class="card-img-wrapper">
            <img src="images/robot-adv.gif" alt="robot" />
          </div>
          <div class="card-info">
            <h3>Robotic Learning</h3>
            <p>
              "Robotics, RL, GAN"
            </p>
            <button><a href="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">Read More</a></button>
          </div>
        </div>
        <div class="card" data-tilt>
          <div class="card-img-wrapper">
            <img src="images/detection.jpeg" alt="detection" />
          </div>
          <div class="card-info">
            <h3>Object Detection</h3>
            <p>
              "Open world, vocabulary, NMS-free"
            </p>
            <button><a href="accv_2016.html">Read More</a></button>
          </div>
        </div>
        <div class="card" data-tilt>
          <div class="card-img-wrapper">
            <img src="images/torchserve.png" alt="detection" />
          </div>
          <div class="card-info">
            <h3>Serverless AI</h3>
            <p>
              "Torchserve, SageMaker, AWS"
            </p>
            <button><a href="http://torchserve-aws-jiali-nodejs.herokuapp.com/">Read More</a></button>
          </div>
        </div>
      </div>  
    </div>  -->

  </div> 
</section>


<!--timeline-->
<section class="section-2" id="section-2">
  <div class="container">
    <div id="timeline">
      <div class="timelineitem">
        <div class="tdate">2023.12 - Present</div>
        <div class="ttitle">Researcher, Apple</div>
        <div class="tdesc">Multi-Modal Generative Modeling</div>
        <div class="tdesc">Cupertino, CA</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2023.08 - 2023.12</div>
        <div class="ttitle">Senior Researcher, Tencent</div>
        <div class="tdesc">Graphics, Image-to-3D</div>
        <div class="tdesc">Palo Alto, CA</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2022.06 - 2023.07</div>
        <div class="ttitle">Research Scientist, FAIR Labs</div>
        <div class="tdesc">Meta AI</div>
        <div class="tdesc">Menlo Park, CA</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2021.06 - 2022.06</div>
        <div class="ttitle">Applied Scientist II, Amazon</div>
        <div class="tdesc">M5 Search Science & AI</div>
        <div class="tdesc">Visual Search & AR</div>
        <div class="tdesc">Palo Alto, CA</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2017.09 - 2021.05</div>
        <div class="ttitle">University of Southern California</div>
        <div class="tdesc">ECE PhD</div>
        <!-- <div class="tdesc"><i>ECE Department</i></div> -->
      </div>
      <div class="timelineitem">
        <div class="tdate">2020.05 - 2021.01</div>
        <div class="ttitle">Amazon Applied Science Intern</div>
        <div class="tdesc">Semi/Self-Supervised Learning</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2019.06 - 2019.09</div>
        <div class="ttitle">Amazon A9 Internship</div>
          <div class="tdesc">Graph-Convolution based Recommendation</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2017.4 - 2017.7 </div>
        <div class="ttitle">Sensetime Internship </div>
          <div class="tdesc">Real-time Portrait Segmentation</div>
      </div>
      <div class="timelineitem">
        <div class="tdate">2015.9 - 2016.3</div>
        <div class="ttitle">AuthenMetic Internship</div>
        <div class="tdesc">Face Detection, Person ReID</div>
      </div>
      <!-- <div class="timelineitem">
      <div class="tdate">2014.9 - 2017.8</div>
        <div class="ttitle">Chinese Academy of Sciences</div>
        <div class="tdesc">National Laboratory of Pattern Recognition</div>
      </div> -->
    </div> 
    <!-- id=timeline -->
  </div>
  <!-- container -->
  <hr class="soft">
</section>

<!-- Recent News -->
<section class="section-3" id="section-3">
<div class="container">
  <ul class="nav nav-tabs">
    <li class="active"><a data-toggle="tab" href="#cv">CV</a></li>
    <li><a data-toggle="tab" href="#projects">Projects</a></li>
  </ul>

  <div class="tab-content">
    <div id="projects" class="tab-pane fade">
        <!-- projects -->
        <div class="container" style="font-size:18px;font-weight:300;margin-bottom:2;">
            <div class="row">
              <br/>

                <div class="col-md-4"> 
                  <div class="pp">
                      <a href="projects/sites/robotic.html">
                        <img src="projects/imgs/robotic-learning.gif" class="imgb">
                      </a>
                  </div>
                  <div class="ppd"><a href="projects/sites/robotic.html">Robotic Learning In Simulation</a></div>
                </div>

                <div class="col-md-4"> 
                    <div class="pp">
                        <a href="projects/sites/vsp.html">
                          <img src="projects/imgs/multimodal-pretrain.gif" class="imgb">
                        </a>
                    </div>
                    <div class="ppd"><a href="projects/sites/vsp.html">Vision-Language Pretraining</a></div>
                </div>

                <div class="col-md-4"> 
                  <div class="pp">
                      <!-- <a href=""> -->
                        <img src="projects/imgs/gpt4.gif" class="imgb">
                      </a>
                  </div>
                  <!-- <div class="ppt">Django+mod_wsgi+apache</div> -->
                  <div class="ppd">Multimodal GPT Study</div> 
              </div>


                
            </div>  <!--row-->

            <div class="row">
              <br/>
                <div class="col-md-4"> 
                    <div class="pp">
                        <!-- <a href=""> -->
                          <img src="projects/imgs/slade-cub-cars.gif" class="imgb">
                        </a>
                    </div>
                    <!-- <div class="ppt">Django+mod_wsgi+apache</div> -->
                    <div class="ppd">Self/Semi-Supervised Metric Learning</div> 
                </div>

                <div class="col-md-4"> 
                    <div class="pp">
                        <!-- <a href=""> -->
                          <img src="projects/imgs/nerf-sdf.gif" class="imgb">
                        </a>
                    </div>
                    <div class="ppd">NeRF & SDF</div>
                </div>

                <div class="col-md-4"> 
                    <div class="pp">
                        <!-- <a href=""> -->
                          <img src="projects/imgs/detect.gif" class="imgb">
                        </a>
                    </div>
                    <div class="ppd">Developments in Detection</div>
                </div>
            </div>  <!--row-->

        </div> <!--outer container-->
    </div> <!--projects-->

    <div id="cv" class="tab-pane fade in active">
      <!-- Personal introduction -->
      <div class="container" style="font-size:18px;font-weight:300;margin-bottom:2;">
        <div class="row">
          <h3>Personal Introduction</h3>
        </div>
            <!-- <p>I'm a Research Scientist at FAIR Labs in Meta AI. My major interests are 3D vision (contributor of Pytorch3d), vision-language representation learning, object detection and robotics. I obtained my PhD at University of Southern California supervised by Prof. <a href="http://mcl.usc.edu/people/cckuo/">C.-C. Jay Kuo</a>. 
            I got my Master degree (Presidential Scholarship) under Prof. <a href="http://www.cbsr.ia.ac.cn/users/szli/">Stan Z. Li</a>,
            <a href="http://www.cbsr.ia.ac.cn/users/scliao/">Shengcai Liao</a> and <a href="http://www.cbsr.ia.ac.cn/users/jwan/">Jun Wan</a>
            at Chinese Academy of Sciences. I have interned at <a href="https://www.sensetime.com/">SenseTime</a> with <a href="https://shijianping.me/">Jianping Shi</a> and <a href="https://www.a9.com/">Amazon Visual Search</a> with Son Tran.--> 
            <p>I am a researcher with a strong track record in Computer Vision and Artificial Intelligence. I am particularly interested in multi-modality learning, foundation models, and generative AI. I am into the research for how these models fundamentally work but also hands on production stacks within the leading industry standards to marriage technology with products. My latest research and engineering focus is on multi-modal generative modelling involving generation and understanding of languages, images, and videos.</p>
            <ol class="breadcrumb"> 
              <li class="breadcrumb-item"><a href="https://davidsonic.github.io/academic_blog/">Blogs at USC</a></li>
              <li class="breadcrumb-item"><a href="https://davidsonic.github.io/">Coding Blogs</a></li>
              <!-- <li class="breadcrumb-item"><a href="https://davidsonic.github.io/Reading_Posts/">Reading Posts</a></li> -->
              <li class="breadcrumb-item"><a href="https://www.linkedin.com/in/jiali-duan-bb5917167/">Linkedin</a></li>
          </ol>
      </div>

      <div class="container" style="font-size:18px;font-weight:300;margin-bottom:2px;">
        <div class="row">
          <h3>Academic Service</h3>
        </div>
        <div>
            <p>
              Associate Editor for APSIPA Transaction on Information and Signal Processing
            </p>
            <p>
              Reviewers: CVPR, ECCV, ICCV, ICML, NeurIPS, ACL, EMNLP, TOMM, RA-L, ICIP
            </p>
          </div>
      </div>

      <div class="container" style="font-size:18px;font-weight:300;margin-bottom:2;">
        <div class="row">
          <h3>Honors</h3>
        </div>
        <div class="mx auto">
              <a href="images/awards/Best-Paper-JVCI.pdf">Best Paper Award JVCI 2021,</a> &nbsp;
              <a href="https://www.iros2019.org/awards">Best Paper Finalist IROS 2019,</a> &nbsp;
              <a href="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">USC Media Coverage 2019,</a>&nbsp;
            <!-- <p>
              Presidential Scholarship, Chinese Academy of Sciences, 2017
            </p> -->
              <a href="images/awards/Best-student-paper-CCBR-2016.pdf">Best Student Paper CCBR 2016</a>
          </div>
      </div>

      <div class="container">
            <div class="row">
              <h3>Recent News</h3>
            </div>
            <div class="container">
                  <div class="news_new_paper">
                    One paper to appear in Neurips 2022!
                  </div>
                  <div class="news_new_code">
                    One paper to appear in ICPR 2022!
                  </div>
                  <div class="news_new_paper">
                    One paper to appear in ICMR 2022 as Oral Presentation!
                  </div>
                  <div class="news_new_code">
                    Two of our papers got accepted to CVPR 2022!
                  </div>
                  <!-- <div class="news_new_paper">
                      Our paper "Interpretable convolutional neural networks via feedforward design" wins Best Paper Award for JVCI, 2021 <a href="http://mcl.usc.edu/people/cckuo/honors-and-awards/">Link</a>
                  </div>
                  <div class="news_new_code">
                      The video, slides and poster for our CVPR 2021 paper are now available!
                  </div> -->
                  <!-- <div class="news_new_paper">
                      Passed PhD thesis defense on March 2nd, 2021 <a href="http://mcl.usc.edu/news/2021/03/07/congratulations-to-jiali-duan-for-passing-his-defense/">Link</a>
                  </div> -->
                  <!-- <div class="news_new_code">
                    Nov. 30, 2019, Express: AI breakthrough: Showing Machine Learning Robots ‘Tough Love’ Helps Them Improve <a href="https://www.express.co.uk/news/science/1210543/ai-artificial-intelligence-machine-learning-robots-tough-love-helps-them-improve">Link</a>
                  </div> -->
                  <!-- <div class="news_new_paper">
                    Nov.06, 2019, Wired: If You Want a Robot to Learn Better, Be a Jerk to It <a href="https://www.wired.com/story/if-you-want-a-robot-to-learn-better-be-a-jerk-to-it/">Link</a>
                  </div>
                  <div class="news_new_code">
                    Nov.06, 2019, Daily Mail: Tough Love! <a href="https://www.dailymail.co.uk/sciencetech/article-7656667/Tug-war-New-research-finds-robots-learn-effectively-humans-provide-physical-resistance.html">Link</a>
                  </div> -->
                  <!-- <div class="news_new_code">
                    USC News: Showing Robots ‘Tough Love’ Helps them Succeed, Finds New USC Study <a href="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">Link</a>
                  </div>
                  <div class="news_new_paper">
                    Our paper is nominated as Best Paper Finalist in IROS 2019
                  </div> -->
          </div>
      </div>
  
    </div>  <!--div cv-->

  </div> <!--tab-content-->
</div> <!--container-->
</section>

  
<section class="section-4" id="section-4">
  <div class="container">
    <h2>Selected Publications</h2>
    <div id="pubs">

      <!--Neurips 22 Paper-->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
  
        <br>
              <img src="papers/nips22/neurips-22-front.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  <!--col-md-6-->
  
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Why do We Need Large Batchsizes in Contrastive Learning? A Gradient-Bias Perspective</b></div>
              <div class="pubd">Contrastive learning (CL) has been the de facto technique for self-supervised representation learning (SSL), with impressive empirical success such as multi-modal representation learning. 
                However, traditional CL loss only considers negative samples from a minibatch, which could cause biased gradients due to the non-decomposibility of the loss. For the first time, we consider optimizing a 
                more generalized contrastive loss, where each data sample is associated with an infinite number of negative samples. We show that directly using minibatch stochastic optimization could lead to gradient bias. 
                To remedy this, we propose an efficient Bayesian data augmentation technique to augment the contrastive loss into a decomposable one, where standard stochastic optimization can be directly applied without gradient bias. 
                Specifically, our augmented loss defines a joint distribution over the model parameters and the augmented parameters, which can be conveniently optimized by a proposed stochastic expectation-maximization algorithm. 
                </div>
  
              <div class="puba"> Changyou Chen, Jianyi Zhang, Yi Xu, Liqun Chen, <span class="pubme">Jiali Duan</span>, Yiran Chen, Son Tran, Belinda Zeng, Trishul Chilimbi</div>
              <div class="pubv">NeurIPS, 2022</div>
              <div class="publ">
                <ul>
                  <li><a href="https://www.amazon.science/publications/why-do-we-need-large-batch-sizes-in-contrastive-learning-a-gradient-bias-perspective">PDF</a></li>
                </ul>
              </div>
  
            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->

      <!--CVPR 22 Paper-->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
  
        <br>
              <img src="papers/cvpr22/codebook.png" style="height:500; width:auto;">
              <img src="papers/cvpr22/grad-cam.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  <!--col-md-6-->
  
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Representation Codebook for Multi-Modal Alignment</b></div>
              <div class="pubd">Aligning signals from different modalities is an important step in 
                vision-language representation learning as it affects the performance of later stages such as cross-modality fusion. 
                Since image and text typically reside in different regions of the feature space, directly aligning them at instance level 
                is challenging especially when features are still evolving during training.
                In this paper, we propose to align at a higher and more stable level using cluster representation. Specifically, 
                we treat image and text as two ``views'' of the same entity, and encode them into a joint vision-language coding space spanned 
                by a dictionary of cluster centers (codebook). We contrast positive and negative samples via their cluster assignments while simultaneously 
                optimizing the cluster centers. To further smooth out the learning process, we adopt a teacher-student distillation paradigm, where the momentum teacher of one view guides the student learning of the other. We evaluated our approach on common vision language benchmarks and obtain new SoTA on zero-shot cross modality retrieval while being competitive on various other transfer tasks.
                </div>
  
              <div class="puba"><span class="pubme">Jiali Duan*</span>, Liqun Chen*, Son Tran, Jinyu Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi</div>
              <div class="pubv">CVPR, 2022</div>
              <div class="publ">
                <ul>
                  <li><a href="https://arxiv.org/abs/2203.00048">PDF</a></li>
                  <li><a href="https://www.amazon.science/publications/multi-modal-alignment-using-representation-codebook">Amazon Science</a></li>
                  <li><a href="cvpr2022/11488-Jiali-poster.pdf">Poster</a></li>
                  <li><a href="cvpr2022/11488-Jiali-CVPRSlides.pdf">Slides</a></li>
                  <li><a href="cvpr2022/CVPR22-11488.mp4">Video</a></li>
                  <li><a href="cvpr2022/codis-short.mp4">Demo</a></li>
                  
                </ul>
              </div>
  
            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->

    <!--CVPR 22 jinyu Paper-->
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

      <br>
            <img src="papers/cvpr22/triple-contrast.png" style="height:500; width:auto;">
            <img src="papers/cvpr22/triple-result.png" style="height:500; width:auto;">
      <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Vision-Language Pre-Training with Triple Contrastive Learning</b></div>
              <div class="pubd">In this paper, we propose triple 
              contrastive learning (TCL) for vision-language pre-training by leveraging both cross-modal and intra-modal self-supervision. Besides CMA, TCL 
              introduces an intra-modal contrastive objective to provide complementary benefits in representation learning. To take advantage of localized and structural 
              information from image and text input, TCL further maximizes the average MI between local regions of image/text and their global summary. To the best of our knowledge, 
              ours is the first work that takes into account local structure information for multi-modality representation learning. Experimental evaluations show that our approach is competitive 
              and achieve the new state of the art on various common down-stream 
              vision-language tasks such as image-text retrieval and visual question answering.
            </div>
            <div class="puba">Jinyu Yang, <span class="pubme">Jiali Duan</span>, Son Tran, Liqun Chen, Yi Xu, Belinda Zeng, Trishul Chilimbi</div>
            <div class="pubv">CVPR, 2022</div>
            <div class="publ">
              <ul>
                <li><a href="https://arxiv.org/abs/2202.10401">PDF</a></li>
                <li><a href="https://www.amazon.science/publications/vision-language-pre-training-with-triple-contrastive-learning">Amzon Science</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
    </div>  <!--col-md-6-->
    </div> <!--row-->
  </div> <!--pubwrap-->

   <!--ICPR 22 Paper-->
   <div class="pubwrap">
    <div class="row">
      <div class="col-md-6">
        <div class="pubimg">

    <br>
          <!-- <img src="icpr22/icpr-framework-sym.png" style="height:500; width:auto;"> -->
          <img src="icpr22/icpr-codebook.png" style="height:500; width:auto;">
    <br><br>
        </div>
      </div>  <!--col-md-6-->

      <div class="col-md-6">
        <div class="pub">
          <div class="pubt"><b>Augmenting Vision Language Pretraining by Learning Codebook with Visual Semantics</b></div>
            <div class="pubd">Language modality within the vision language pre
              training framework is innately discretized, endowing each word in
              the language vocabulary a semantic meaning. In contrast, visual
              modality is inherently continuous and high-dimensional, which
              potentially prohibits the alignment as well as fusion between vision
              and language modalities. We therefore propose to “discretize” the
              visual representation by joint learning a codebook that imbues
              each visual token a semantic. We then utilize these discretized
              visual semantics as self-supervised ground-truths for building
              our Masked Image Modeling objective, a counterpart of Masked
              Language Modeling which proves successful for language models.
              To optimize the codebook, we extend the formulation of VQ-
              VAE which gives a theoretic guarantee. Experiments validate
              the effectiveness of our approach across common vision-language
              benchmarks.
          </div>
          <div class="puba">Xiaoyuan Guo*, <span class="pubme">Jiali Duan*</span>, C.-C. Jay Kuo, Judy Wawira Gichoya, and Imon Banerjee</div>
          <div class="pubv">ICPR, 2022</div>
          <div class="publ">
            <ul>
              <li><a href="icpr22/ICPR22.pdf">PDF</a></li>
            </ul>
          </div>

        </div>  <!--pub-->
  </div>  <!--col-md-6-->
  </div> <!--row-->
</div> <!--pubwrap-->

    <!--JMLR 22 Paper-->
    <!-- <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

      <br>
            <img src="papers/JMLR/plots.png" style="height:500; width:auto;">
      <br><br>
          </div>
        </div>  

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Optimality and Convergence of Natural Policy Gradient Primal-Dual Method for Constrained MDPs</b></div>
            <div class="pubd">
              We study sequential decision-making problems in which each agent aims to maximize the expected total reward while satisfying 
              a constraint on the expected total utility. We employ the natural policy gradient method to solve the discounted infinite-horizon 
              Constrained Markov Decision Processes (constrained MDPs) problem. Specifically, we propose a new Natural Policy Gradient Primal-Dual (NPG-PD) 
              method for constrained MDPs which updates the primal variable via natural policy gradient ascent and the dual variable via projected sub-gradient descent. 
              Even though the underlying maximization involves a nonconcave objective function and a nonconvex constraint set under the softmax policy parametrization, 
              we prove that our method achieves global convergence with sublinear rates regarding both the optimality gap and the constraint violation. To the best of our knowledge, our work is the first to establish non-asymptotic convergence 
              guarantees of policy-based primal-dual methods for solving infinite-horizon discounted constrained MDPs.
              We also provide computational experiments to showcase the effectiveness of our approach.
              </div>

            <div class="puba">Dongsheng Ding, Kaiqing Zhang, <span class="pubme">Jiali Duan</span>, Tamer Bacsar, Mihailo R. Jovanovic</div>
            <div class="pubv">JMLR Under Revision, 2021</div>
            <div class="publ">
              <ul>
              </ul>
            </div>

          </div>
    </div>  
    </div> 
  </div> -->

        <!--Survey Paper-->
        <!-- <div class="pubwrap">
          <div class="row">
            <div class="col-md-6">
              <div class="pubimg">
    
          <br>
                <img src="papers/bridge-semantics/semantics.png" style="height:500; width:auto;">
                <img src="papers/bridge-semantics/apsipa-survey.png" style="height:500; width:auto;">
          <br><br>
              </div>
            </div>  
    
            <div class="col-md-6">
              <div class="pub">
                <div class="pubt"><b>Bridging Gap between Image Pixels and 
                  Semantics via Supervision: A Survey</b></div>
                <div class="pubd">The fact that there exists a gap between low-level features and semantic
                  meanings of images, called the semantic gap, is known for decades.
                  Resolution of the semantic gap is a long standing problem.  The semantic
                  gap problem is reviewed and a survey on recent efforts in bridging the
                  gap is made in this work. Most importantly, we claim that the semantic
                  gap is primarily bridged through supervised learning today.  Experiences
                  are drawn from two application domains to illustrate this point: 1)
                  object detection and 2) metric learning for content-based image
                  retrieval (CBIR). To begin with, this paper offers a historical
                  retrospective on supervision, makes a gradual transition to the modern
                  data-driven methodology and introduces commonly used datasets.  Then, it
                  summarizes various supervision methods to bridge the semantic gap in the
                  context of object detection and metric learning. 
                  </div>
    
                <div class="puba"><span class="pubme">Jiali Duan</span>, C.-C. Jay Kuo</div>
                <div class="pubv">APSIPA, 2021</div>
                <div class="publ">
                  <ul>
                    <li><a href="https://arxiv.org/abs/2107.13757">PDF</a></li>
                  </ul>
                </div>
    
              </div>  
        </div> 
        </div> 
      </div>  -->

        <!--CVPR2021-->
        <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="SLADE/cover_image.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  <!--col-md-6-->

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>SLADE: A Self-Training Framework For Distance Metric Learning</b></div>
              <div class="pubd">Most existing distance metric learning approaches use fully labeled data to learn the sample similarities in an embedding space. We present a
                  self-training framework, <i>SLADE</i>, to improve retrieval performance by leveraging additional unlabeled data. We first train a teacher model on the labeled
                  data and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate final feature embeddings.
                  We use self-supervised representation learning to initialize the teacher model. To better deal with noisy pseudo labels generated by the teacher network, we design a
                  new feature basis learning component for the student network, which learns basis functions of feature representations for unlabeled data. The learned basis vectors better
                  measure the pairwise similarity and are used to select high-confident samples for training the student network. We evaluate our method on standard retrieval
                  benchmarks: CUB-200, Cars-196 and In-shop. Experimental results demonstrate that our approach significantly improves the performance over the state-of-the-art methods.</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Yen-Liang Lin, Son Tran, Larry Davis and C.-C. Jay Kuo</div>
              <div class="pubv">CVPR 2021</div>
              <div class="publ">
                <ul>
                  <li><a href="https://arxiv.org/abs/2011.10269">PDF</a></li>
                  <li><a href="https://www.amazon.science/publications/slade-a-self-training-framework-for-distance-metric-learning">Amazon Blog</a></li>
                  <li><a href="cvpr2021/3886-Jiali-CVPRPoster.pdf">Poster</a></li>
                  <li><a href="cvpr2021/3886-Jiali-CVPRSlides.pdf">Slides</a></li>
                  <li><a href="cvpr2021/3886-Jiali-CVPRVideo.mp4">Video</a></li>
                  <li><a href="SLADE/cars196-demo.html">Cars196-demo</a></li>
                  <li><a href="SLADE/CUB200-demo.html">CUB200-demo</a></li>
                </ul>
              </div>

            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->
        <!--CVPR2021-->


        <!--RL-->
        <!-- <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="papers/curri/curri-rl-cover.png" style="height:500; width:auto;">
              <img src="papers/curri/curri-rl-env.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Challenge Reinforcement Learning with Interactive Curriculum</b></div>
              <div class="pubd">Curriculum reinforcement learning benefits training by focusing on examples of gradual increasing difficulty
                  that are neither too hard nor too easy. However, it's not always intuitive to define a curriculum that lies within this ``range of interest''.
                  On one end of spectrum, a SOTA algorithm that learns from scratch may fail to collect any reinforcing signal. While on the other end,
                  an automatic task-agnostic curriculum could ``overfit'' in an early phase that prevents it from further adaptation. In contrast,
                  human has an innate ability to improvise and adapt when confronted with different scenarios, which we utilize to provide
                  explainability and guidance for curriculum reinforcement learning. We first identify the ``inertial'' problem in automatic curriculum
                  and then propose a simple interactive curriculum framework that works in environments that require millions of interactions.</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Yilei Zeng, Yang Li, Emilio Ferrara, Lerrel Pinto, C.-C. Jay Kuo, Stefanos Nikolaidis</div>
              <div class="pubv">Preprint, 2021</div>
              <div class="publ">
                <ul>
                  <li><a href="papers/curri/curriculum-rl.pdf">PDF</a></li>
                  <li><a href="https://github.com/davidsonic/interactive-curriculum-reinforcement-learning">Demo (Github)</a></li>
                  <li><a href="interactive-rl/tutorial-interact.mp4">Training</a></li>
                  <li><a href="interactive-rl/interactive-demo-jump.mp4">Interactive-Jumper</a></li>
                  <li><a href="interactive-rl/interactive-demo-crawler.mp4">Interactive-Crawler</a></li>
                </ul>
              </div>

            </div>  
      </div>  
      </div> 
    </div>  -->
        <!--RL-->

        <!--SCMLS-->
        <!-- <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="papers/scmls/tsne.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Fashion Compatibility Recommendation via Unsupervised Metric Graph Learning</b></div>
              <div class="pubd">In the task of fashion compatibility prediction, the goal is to pick an item from a candidate list to complement a partial
                  outfit in the most appealing manner. Existing fashion compatibility recommendation work comprehends clothing images in a single metric space and lacks
                  detailed understanding of users’ preferences in different contexts. To address this problem, we propose a novel Metric-Aware Explainable Graph Network (MAEG).
                  In MAEG, we propose an unsupervised approach to obtain representation of items in a metric-aware latent semantic space. Then, we develop a graph filtering network and Pairwise Preference
                  ttention module to model the interactions between users’ preferences and contextual information. Experiments on real world dataset reveals that MAEG not only outperforms
                  the state-of-the-art methods, but also provides interpretable insights by highlighting the role of semantic attributes and contextual relationships among items.</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Xiaoyuan Guo, Son Tran and C.-C. Jay Kuo</div>
                <div class="pubv">SCMLS 2020</a></div>
              <div class="publ">
                <ul>
                  <li><a href="papers/fashion-graph.pdf">Preprint</a></li>
                  <li><a href="papers/scmls/scmls20_paper_27.pdf">SCMLS</a></li>
                  <li><a href="compat/PolyvoreOutfits_fitb.pdf">FITB-DEMO</a></li>
                  <li><a href="compat/PolyvoreOutfits_compat.pdf">Compat-DEMO</a></li>
                </ul>
              </div>

            </div>  
      </div>  
      </div> 
    </div>  -->
        <!--SCMLS-->

            <!--WACV PortraitGAN-->
        <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="wacv/framework2.png" style="height:500; width:auto;">
              <img src="wacv/interact.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  <!--col-md-6-->

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>PortraitGAN for Flexible Portrait Manipulation</b></div>
              <div class="pubd">Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of
                  canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports
                  continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency
                  into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces
                  bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure
                  high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates
                  gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality
                  portrait manipulation with photo-realistic effects. </div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Xiaoyuan Guo, and C.-C. Jay Kuo</div>
              <div class="pubv">APSIPA 2020</div>
              <div class="publ">
                <ul>
                  <li><a href="papers/PortraitGAN-final.pdf">PDF</a></li>
                  <!-- <li><a href="https://github.com/davidsonic/Flexible-Portrait-Manipulation">Code (Coming Soon)</a></li> -->
                  <li><a href="APSIPA/APSIPA_GAN-9-12.pdf">Visualization</a></li>
                </ul>
              </div>

            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->
        <!--PortraitGAN-->


        <!--IROS-->
        <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="scr/iros.png" style="height:500; width:auto;">
              <img src="scr/grasp_demo1.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  <!--col-md-6-->

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt" id="IROS19"><b>Robot Learning via Human Adversarial Games</b></div>
              <div class="pubd">Much work in robotics has focused on “human-in-the-loop” learning techniques that improve the efficiency of
  the learning process. However, these algorithms have made the
  strong assumption of a cooperating human supervisor that assists
  the robot. In reality, human observers tend to also act in an
  adversarial manner towards deployed robotic systems. We show
  that this can in fact improve the robustness of the learned models
  by proposing a physical framework that leverages perturbations
  applied by a human adversary, guiding the robot towards more
  robust models. In a manipulation task, we show that grasping
  success improves significantly when the robot trains with a
  human adversary as compared to training in a self-supervised
  manner.  </div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Qian Wang, Lerrel Pinto, C.-C. Jay Kuo, and Stefanos Nikolaidis</div>
                <div class="pubv">IROS 2019 (Best Paper Finalist), <a href="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">USC Media</a></div>
              <div class="publ">
                <ul>
                  <li><a href="https://arxiv.org/abs/1903.00636">PDF</a></li>
                  <li><a href="IROS/before-train.mp4">Before-Train</a></li>
                  <li><a href="IROS/after-train.mp4">After-Train</a></li>
                  <li><a href="interactive-rl/tutorial-interact.mp4">Training</a></li>
                  <li><a href="interactive-rl/interactive-demo-jump.mp4">Jumper</a></li>
                  <li><a href="interactive-rl/interactive-demo-crawler.mp4">Crawler</a></li>
                </ul>
              </div>

            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->
        <!--IROS-->

        <!--SCR-->
        <!-- <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="scr/iros2019-1.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Robust Grasping via Human Adversary</b></div>
              <div class="pubd">An interactive interface has been developed based on customized Mujoco, where human perturbation is applied to guide the learning of
              robust grasping behavior. We compared human adversary with simulated adversary and grasping policy learned in a self-supervised manner. Both quantitative and subjective user studies verify
              the effectiveness of human domain knowledge involved. We're the first work to study the effect of human-adversary in robotic learning.</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Qian Wang, Lerrel Pinto, C.-C. Jay Kuo, and Stefanos Nikolaidis</div>
              <div class="pubv">SCR 2019 (Spotlight)</div>
              <div class="publ">
                <ul>
                  <li><a href="https://davidsonic.github.io/summary/caltech_symposium.pdf">PDF</a></li>
                </ul>
              </div>

            </div>  
      </div>  
      </div> 
    </div>  -->
        <!--SCR-->

        <!--Interpret-->
        <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="saak/Saak-pic1.png" style="height:500; width:auto;">
              <img src="saak/Saak-pic2.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div>  <!--col-md-6-->

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Interpretable Convolutional Neural Networks via Feedforward Design</b></div>
              <div class="pubd">The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable
  feedforward (FF) design without any BP as a reference. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in
  a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace approximation with adjusted bias)
  transform. It is a variant of the principal component analysis (PCA) with an added bias vector to annihilate activation’s nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected
  (FC) layers, we construct them using a cascade of multi-stage linear least squared regressors (LSRs). The classification and robustness (against adversarial attacks) performances of BP- and FF-designed CNNs applied to the
  MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.</div>

              <div class="puba">C.-C. Jay Kuo, Min Zhang, Siyang Li, <span class="pubme">Jiali Duan</span> and Yueru Chen</div>
              <div class="pubv">JVCI 2019 (Best Paper Award)</div>
              <div class="publ">
                <ul>
                  <li><a href="https://arxiv.org/pdf/1810.02786.pdf">PDF</a></li>
                </ul>
              </div>

            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->
        <!--Interpret-->

        <!--Yzhu-->
        <!-- <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">

        <br>
              <img src="saak/yzhu1.png" style="height:500; width:auto;">
        <br><br>
            </div>
          </div> 

          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>An Interpretable Generative Model for Handwritten Digit Image Synthesis</b></div>
              <div class="pubd">Modern image generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs),
  are trained by backpropagation (BP). The training process is complex and the underlying mechanism is difficult to explain.
                  We propose an interpretable generative model for handwritten digits synthesis is proposed in this work.</div>
              <div class="puba">Yao Zhu, Saksham Suri, Pranav Kulkarni, Yueru Chen, <span class="pubme">Jiali Duan</span> and C.-C. Jay Kuo</div>
              <div class="pubv">ICIP 2019</div>
              <div class="publ">
                <ul>
                  <li><a href="https://arxiv.org/pdf/1811.04507.pdf">PDF</a></li>
                </ul>
              </div>

            </div>  
      </div>  
      </div> 
    </div>  -->
        <!--Yzhu-->

      
    <!--ACM-TOMM 2017-->
    
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
              
        <br>
              <img src="ACM-TOMM/acm-tomm-gesture.png" style="height:500; width:auto;"> 
        <br><br>
            </div>  
          </div>  <!--col-md-6-->
        
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>A Unified Framework for Multi-Modal Isolated Gesture Recognition</b></div>
              <div class="pubd">In this paper, we focus on isolated gesture recognition and explore different modalities by involving
  RGB stream, depth stream and saliency stream for inspection. Our goal is to push the boundary of this
  realm even further by proposing a unified framework which exploits the advantages of multi-modality
  fusion. Specifically, a spatial-temporal network architecture based on consensus-voting has been pro-
  posed to explicitly model the long term structure of the video sequence and to reduce estimation vari-
  ance when confronted with comprehensive inter-class variations. In addition, a 3D depth-saliency con-
  volutional network is aggregated in parallel to capture subtle motion characteristics.</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Jun Wan, Shuai Zhou, Xiaoyuan Guo, and Stan Z. Li</div>
              <div class="pubv">ACM-TOMM 2017 </div>
              <div class="publ">
                <ul>
                  <li><a href="ACM-TOMM/ACM-TOMM.pdf">PDF</a></li>
                </ul>
              </div>
              
            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->

      <!--cvpr 2017-->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
              

              <img src="cvpr/cvpr-2SCVN.png" style="height:200px; width:auto;"> 
        <br><br>
        <img src="cvpr/cvpr-result-table.png" style="height:150px; width:auto;" >
            </div>  
          </div>  <!--col-md-6-->
        
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for
  Isolated Gesture Recognition</b></div>
              <div class="pubd"> We propose a convolutional two-stream consensus voting network (2SCVN) which explicitly models both the short-term and long-term structure of
  the RGB sequences. To alleviate distractions from background, a 3d depth-saliency ConvNet stream (3DDSN) is
  aggregated in parallel to identify subtle motion characteristics. These two components in an unified framework significantly improve the recognition accuracy. On the challenging Chalearn IsoGD benchmark, our proposed method
  outperforms the first place on the leader-board by a large
  margin (10.29%) while also achieving the best result on
  RGBD-HuDaAct dataset (96.74%).</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Shuai Zhou, Jun Wan, Xiaoyuan Guo, and Stan Z. Li</div>
              <div class="pubv">Technical Report, 2016</div>
              <div class="publ">
                <ul>
                  <li><a href="https://arxiv.org/pdf/1611.06689v1.pdf">PDF</a></li>
                </ul>
              </div>
              
            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->


      <!--ccbr 2016-->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
              

              <img src="ccbr/results.png" style="height:200px; width:auto;"> 
              <img src="ccbr/enlarged.png" style="height:200px; width:auto;" >	
        <br><br>
        <img src="ccbr/table1.png" style="height:100px; width:auto;" >
            </div>  
          </div>  <!--col-md-6-->
        
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Face Classification: A Specialized Benchmark Study</b></div>
              <div class="pubd"> We conduct a specialized
  benchmark study in this paper, which focuses on face classifica
  tion. We start with face proposals, and build a benchmark dataset with
  about 3.5 million patches for two-class face/non-face classification. Results with several baseline algorithms show that, without the help of
  post-processing, the performance of face classification itself is still not
  very satisfactory, even with a powerful CNN method. We’ll release this
  benchmark to help assess performance of face classification only, and ease
  the participation of other related researchers.</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Shengcai Liao, Shuai Zhou, and Stan Z. Li</div>
              <div class="pubv">CCBR 2016 (Best Student Paper) </div>
              <div class="publ">
                <ul>
                  <li><a href="ccbr_2016.html">Project</a></li>
                  <li><a href="ccbr/ccbr2016.pdf">PDF</a></li>
              <li><a href="https://github.com/davidsonic/face_classification_ccbr2016">Code (Github)</a></li>
                  <li><a href="ccbr/FCB-CCBR2016.pptx">Presentation</a></li>
                </ul>
              </div>
              
            </div>  <!--pub-->
      </div>  <!--col-md-6-->
      </div> <!--row-->
    </div> <!--pubwrap-->


  <!--ACCV Workshop-->
      <div class="pubwrap">
        <div class="row">
          <div class="col-md-6">
            <div class="pubimg">
              

              <img src="accv/AFW_NEW_SMALL.png" style="height:200px; ">
              <img src="accv/FDDB_NEW_SMALL.png" style="height:200px;">
        
            </div>  
          </div>  <!--col-md-6-->
        
          <div class="col-md-6">
            <div class="pub">
              <div class="pubt"><b>Face Detection by Aggregating Visible Components</b></div>
              <div class="pubd">In this paper, we propose a novel face detection method called
  Aggregating Visible Components (AVC), which addresses pose variations
  and occlusions simultaneously in a single framework with low complexi-
  ty. The main contributions of this paper are: (1) By aggregating visible
  components which have inherent advantages in occasions of occlusions,
  the proposed method achieves state-of-the-art performance using only
  hand-crafted feature; (2) Mapped from meanshape through component-
  invariant mapping, the proposed component detector is more robust to
  pose-variations (3) A local to global aggregation strategy that involves
  region competition helps alleviate false alarms while enhancing localiza-
  tion accuracy.	</div>

              <div class="puba"><span class="pubme">Jiali Duan</span>, Shengcai Liao, Xiaoyuan Guo, and Stan Z. Li</div>
              <div class="pubv">ACCV Workshop 2016 (Oral)</div>
              <div class="publ">
                <ul>
                  <li><a href="accv_2016.html">Project</a></li>
                  <li><a href="accv/accv2016finalpaper.pdf">PDF</a></li>
                  <li><a href="accv/WFI-ACCV2016.pptx">Presentation (+audio)</a>
                </ul>
              </div>
              
            </div>  <!--pub-->
          </div> <!--col-md-6--> 
    <br><br>
          <div style="text-align:center;">
    <img src="accv/pipeline.png" style="width:80%;"> 	
    </div>
        </div>  <!--row-->
          
      </div>  <!--pubwrap-->
        

    </div> <!--pubs-->
  </div>  <!--container-->
</section>

<section class="section-5" id="section-5">
  <div class="container">
      <h2>Activity</h2>
      <div class="ctr">
          <div class="hht">
            Autumn 5th/November/2014: I was awarded second-place for <br>
              <a href="http://news1.ucas.ac.cn/Home/Detail18/16df15f8-cc21-4350-8c9f-e54ace93a0ce"> English Speaking Competition held by University of Chinese Academy of Sciences </a>
          </div>
          <a href="images/speech-ucas.png"> 
          <img src="images/ucas-speaking.jpg" style="width:60%"></a>
          <div>
            <audio controls>
                <source src="speech.mp3" type="audio/mpeg">
            </audio>
          </div>
      </div>

      <div class="ctr">
          <div class="hht">
              Winter December/2012: Shanghai Final on behalf of ECUST for <br>
              <a href="">
                  21st Century Coca-Cola Cup National English Speaking Competition</a> 
          </div>
          <a href="images/outstanding.jpg"> 
          <img src="images/shanghai-english-pic.jpg" style="width:60%"></a>
      </div>

      <div class="ctr">
          <div class="hht">
            2012/2013 I was awarded with Second prize and honorable mention for <br>
                  <a href="images/math-china.jpg">
                      National Mathematical Contest in Modeling</a> and
            <a href="images/math-US.png">MCM/ICM</a> respectively
          </div>
      </div>

      <!--<div class="row">-->
    <!--<div class="col-md-6">-->
      <!--<div style="font-size:25px;">Academic</div>-->

        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/beijing-english.png">Honarable Mention in Beijing English Speaking Competition for Master Student&ndash;&gt;-->
          <!--<a href="">Honarable Mention in Beijing English Speaking Competition for Master Student-->
          <!--</a>-->
        <!--</div>-->

        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/shanghai-tech.jpg">Participate in Summer Camp for Recommended Student for Admission of ShanghaiTech University</a>&ndash;&gt;-->
          <!--<a href="">Participate in Summer Camp for Recommended Student for Admission of ShanghaiTech University</a>-->
        <!--</div>-->

          <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/math-US.png">Honarable Mention for MCM/ICM Mathematical Modeling Contest for American Students</a>&ndash;&gt;-->
          <!--<a href="">Honarable Mention for MCM/ICM Mathematical Modeling Contest for American Students</a>-->
        <!--</div>-->

        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/math-china.jpg">Second Prize for National Mathematical Contest in Modeling&ndash;&gt;-->
          <!--<a href="">Second Prize for National Mathematical Contest in Modeling-->
          <!--</a>-->
        <!--</div>-->
        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/math-shanghai.jpg">First Prize for Shanghai Mathematical Contest in Modeling&ndash;&gt;-->
          <!--<a href="">First Prize for Shanghai Mathematical Contest in Modeling-->
          <!--</a>-->
        <!--</div>-->

          <!---->

    <!--</div>-->

    <!--<div class="col-md-6">-->
      <!--<div style="font-size:25px;">Certificates</div>-->
        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/Advanced-interpretation.jpg">Adavanced-Level English Interpretation Accreditation Examination Certificate&ndash;&gt;-->
          <!--<a href="">Adavanced-Level English Interpretation Accreditation Examination Certificate-->
          <!--</a>-->
        <!--</div>-->


        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/exe-student1.png">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2014-2015&ndash;&gt;-->
          <!--<a href="">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2014-2015-->
          <!--</a>-->
        <!--</div>-->

        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/exe-student2.png">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2015-2016&ndash;&gt;-->
          <!--<a href="">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2015-2016-->
          <!--</a>-->
        <!--</div>-->
        <!---->

        <!--<div class="ts">-->
          <!--&lt;!&ndash;<a href="images/volunteer.png">Volunteer and Referee for the 16th China Adolescent Robotics Competition&ndash;&gt;-->
          <!--<a href="">Volunteer and Referee for the 16th China Adolescent Robotics Competition-->
          <!--</a>-->
        <!--</div>-->
      <!---->

    <!--</div>-->
      <!--</div>  &lt;!&ndash;row&ndash;&gt;-->

      <!--show more-->
      <!-- <div class="showmore" id="showmorepubs">show more</div>
      <div id="morepubs">
        <div class="row">
          <div class="col-md-6">

            <div class="ts">
            <a href="images/2011-runner-up.jpg">Second Prize for 2011 English Debating Competition in ECUST
            </a>
          </div>

            <div class="ts">
            <a href="images/uchallenge-2nd.jpg">Second Prize for 2011 Uchallenge English Speaking Competition
            </a>
          </div>

            <div class="ts">
            <a href="images/ECUST-1st.jpg">First Prize for 2011 Extemporaneous English Speaking Competition in ECUST
            </a>
          </div>

            <div class="ts">
            <a href="images/electronic-comp.jpg">Second Prize for <i> Electronic Assembly-Debugging and Developing Competition</i> in ECUST
            </a>
          </div>

          <div class="ts">
            <a href="images/new-ecust.jpg">First Prize for <i> English Speaking Competition for New Students</i> in ECUST
            </a>
          </div>

          <div class="ts">
            <a href="images/scholarship-ecust.jpg">Scholarships Awarded in ECUST
            </a>
          </div>

          </div>  

        </div> 
      </div> morepubs -->

  </div>  <!--container -->
  
</section>


<!-- <hr> -->
<section class="section-6" id="section-6">
  <p class="copyright">jli.duan@gmail.com &copy; Last updated Jan 2024</p>
</section>


<!-- </section> -->

<!-- Javascript -->
  <!--place js at the end-->
  <script src="js/bootstrap.min.js"></script>
  <script src="js/jquery-1.11.1.min.js"></script>
  <script src="js/script.js"></script>
  <script>
    var more_projects_shown = false;
    $(document).ready(function() {
      $("#showmoreprojects").click(function() {
        if(!more_projects_shown) {
          $("#moreprojects").slideDown('fast', function() {
            $("#showmoreprojects").text('hide');
          });
          more_projects_shown = true;
        } else {
          $("#moreprojects").slideUp('fast', function() {
            $("#showmoreprojects").text('show more');
          });
          more_projects_shown = false;
        }
      });

      var more_pubs_shown = false;
      $("#showmorepubs").click(function() {
        if(!more_pubs_shown) {
          $("#morepubs").slideDown('fast', function() {
            $("#showmorepubs").text('hide');
          });
          more_pubs_shown = true;
        } else {
          $("#morepubs").slideUp('fast', function() {
            $("#showmorepubs").text('show more');
          });
          more_pubs_shown = false;
        }
        });
    });
  //
  </script>
  <script src="js/tilt.js"></script>
  <script src="js/sidebar.js"></script>
  </body>
</html>
