<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <meta name="description" content="Duanjiali.GitHub.io : Jiali Duan&#39;s Academic">
    <link href="css/bootstrap.min.css" rel="stylesheet" media="screen">
    <link href="style.css" rel="stylesheet">
    <link href="extra.css" rel="stylesheet">
    <link href='http://fonts.googleapis.com/css?family=Roboto:400,300,500' rel='stylesheet' type='text/css'>
    <link href='stylesheets/github-light.css' rel='stylesheet' type='text/css'>
    <link href='stylesheets/sytlesheet.css' rel='stylesheet' type='text/css'>
    <script src="js/jquery-1.11.1.min.js"></script>
    <title>Jiali Duan's Academic Site</title>
  </head>

  <body>

    <!-- HEADER -->
<div id="header" class="bg1">
  <div id="topnav">
    <ul>
      <li><a href="https://davidsonic.github.io/academic_blog/">Blog</a></li>
    </ul>
  </div>
  <div id="headerblob">
    <img src="images/djl3.jpg" class="img-circle imgme" style="height:200px; width=250px;">
    <div id="headertext">
      <div id="htname">Jiali Duan</div>
      <div id="htdesc">PhD Candidate (Supervised by C.-C. Jay Kuo)</div>
      <div id="htdesc">University of Southern California</div>
      <div id="htem">jialidua@usc.edu</div>
    </div>
  </div>
</div>


<!--timeline-->
<div class="container">
    <div id="timeline">
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2020.01- 2020.03</div>-->
      <!--<div class="ttitle">Interactive Curriculum Reinforcement Learning</div>-->
    <!--</div>-->
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2019.09- 2019.12</div>-->
      <!--<div class="ttitle">Fashion Compatibility Prediction</div>-->
        <!--<div class="tdesc"><i>Explainable Graph Network for Recommendation</i><br> <a href="https://cseweb.ucsd.edu/~jmcauley/workshops/scmls20/">SCMLS 2020</a></div>-->
    <!--</div>-->
    <div class="timelineitem">
      <div class="tdate">2020.05- 2021.01</div>
      <div class="ttitle">Amazon A9 Internship</div>
        <div class="tdesc"><i>Applied Science Intern</i><br><i>Semi/Self-Supervised Learning</i></div>
    </div>
    <div class="timelineitem">
      <div class="tdate">2019.06- 2019.09</div>
      <div class="ttitle">Amazon A9 Internship</div>
        <div class="tdesc"><i>Applied Science Intern</i><br><i>Graph-Convolution based Recommendation</i></div>
    </div>
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2019.01- 2019.03</div>-->
      <!--<div class="ttitle">Interactive Human-Robot Learning</div>-->
      <!--<div class="tdesc"><i>Robot Learning via Human Adversarial Games</i><br>IROS 2019 (<a href="https://arxiv.org/abs/1903.00636">Best Paper Finalist</a>,<a href ="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">USC Coverage</a>)</div>-->
    <!--</div>-->
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2018.12- 2019.03</div>-->
      <!--<div class="ttitle">Robust Grasping</div>-->
      <!--<div class="tdesc"><i>Robust Grasping via Human Adversary</i><br>SCR 2019 (<a href="http://scr2019.caltech.edu/">Spotlight</a>)</div>-->
    <!--</div>-->
	<!--<div class="timelineitem">-->
	  <!--<div class="tdate">2018.1 -2018.4 </div>-->
	  <!--<div class="ttitle">Generative Adversarial Network</div>-->
	  <!--<div class="tdesc"><i> PortraitGAN for Flexible Portrait Manipulation</i> <br> <a href="papers/PortraitGAN-final.pdf"> APSIPA 2020</a> </div>-->
	<!--</div>-->
    <div class="timelineitem">
      <div class="tdate">2017.09 - 2021.05</div>
      <div class="ttitle">University of Southern California</div>
      <div class="ttitle">PhD Degree</div>
      <div class="tdesc"><i>ECE Department</i></div>
    </div>
	<div class="timelineitem">
	  <div class="tdate">2017.4 -2017.7 </div>
	  <div class="ttitle">Sensetime Internship </div>
      <div class="tdesc"><i> Real-time Portrait Segmentation</i></div>
	</div>
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2016.12 - 2017.1 </div>-->
      <!--<div class="ttitle">Depth Video Analysis</div>-->
      <!--<div class="tdesc"><i>A Unified Framework for Multi-Modal <br>Isolated Gesture Recognition</i><br><a href="ACM-TOMM/ACM-TOMM.pdf">ACM-TOMM 2017</a></div>-->
      <!--</div>-->
    <!--<div class="timelineitem">-->
   	  <!--<div class="tdate">2016.9 - 2016.11 </div>-->
      <!--<div class="ttitle">Video Recognition</div>-->
      <!--<div class="tdesc"><i>Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for Isolated Gesture Recognition</i><br> <a href="https://arxiv.org/pdf/1611.06689v2.pdf">arxiv</a></div>-->
      <!--</div>-->
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2016.6 - 2016.7</div>-->
      <!--<div class="ttitle">Face classification benchmark</div>-->
      <!--<div class="tdesc"><i>Face Classification: A Specialized Benchmark Study</i> <br> CCBR 2016 (<a href="ccbr/Best Student Paper.pdf">Best Student Paper</a>)</div>-->
    <!--</div>-->
    <!--<div class="timelineitem">-->
      <!--<div class="tdate">2016.3 - 2016.6</div>-->
      <!--<div class="ttitle">Face detection</div>-->
      <!--<div class="tdesc"><i>Face Detection by Aggregating Visible Components</i> <br> ACCV Workshop 2016 (<a href="http://www2.docm.mmu.ac.uk/STAFF/m.yap/programme.php">Oral</a>) </div>-->
    <!--</div>-->
    <div class="timelineitem">
      <div class="tdate">2015.9 - 2016.3</div>
      <div class="ttitle">AuthenMetic Internship</div>
      <div class="tdesc">Face Detection, Person ReID</div>
    </div>
    <div class="timelineitem">
     <div class="tdate">2014.9 - 2017.8</div>
      <div class="ttitle">Chinese Academy of Sciences</div>
      <div class="ttitle">Master Degree</div>
      <div class="tdesc">National Laboratory of Pattern Recognition</div>
    </div>
    </div>
</div>
  
  
<!--publication-->
          
<div class="container" style="font-size:18px; font-weight:300;margin-top:50px;margin-bottom:50px;">
  
<b>Personal Introduction</b>
<p>I'm currently a PhD at University of Southern California under the supervision of Prof. <a href="http://mcl.usc.edu/people/cckuo/">C.-C. Jay Kuo</a>.
    Before that, I completed my master degree with honor (Presidential Scholarship) under the guidance of Prof. <a href="http://www.cbsr.ia.ac.cn/users/szli/">Stan Z. Li</a>,
    <a href="http://www.cbsr.ia.ac.cn/users/scliao/">Shengcai Liao</a> and <a href="http://www.cbsr.ia.ac.cn/users/jwan/">Jun Wan</a>
    at Institute of Automation, Chinese Academy of Sciences. I have also interned at <a href="http://www.authenmetric.com/">AuthenMetric</a>
    , <a href="https://www.sensetime.com/">SenseTime</a> working with Dr. Jianping Shi</a> and <a href="https://www.a9.com/">Amazon A9</a> with Principal Applied Scientist Son Tran. My personal blog is <a href="https://davidsonic.github.io/academic_blog/">here</a> and I love to participate in competitive programming contests during my spare time.</p>

</div>


<hr class="soft">
<div class="container">
  <div class="row">
    <h3>Media Converage</h3>
    </div>
        <div class="news_new_paper">
          Nov. 30, 2019, Express: AI breakthrough: Showing Machine Learning Robots ‘Tough Love’ Helps Them Improve <a href="https://www.express.co.uk/news/science/1210543/ai-artificial-intelligence-machine-learning-robots-tough-love-helps-them-improve">Link</a>
        </div>
        <div class="news_new_code">
          Nov.06, 2019, Wired: If You Want a Robot to Learn Better, Be a Jerk to It <a href="https://www.wired.com/story/if-you-want-a-robot-to-learn-better-be-a-jerk-to-it/">Link</a>
        </div>
        <div class="news_new_paper">
          Nov.06, 2019, Daily Mail: Tough Love! <a href="https://www.dailymail.co.uk/sciencetech/article-7656667/Tug-war-New-research-finds-robots-learn-effectively-humans-provide-physical-resistance.html">Link</a>
        </div>
        <div class="news_new_code">
          Nov.06, 2019, USC News: Showing Robots ‘Tough Love’ Helps them Succeed, Finds New USC Study <a href="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">Link</a>
  </div>
</div>
<hr class="soft">


<div class="container">
  <h2>Publications</h2>
  <div id="pubs">
      <!--CVPR2021-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="SLADE/cover_image.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>SLADE: A Self-Training Framework For Distance Metric Learning</b></div>
            <div class="pubd">Most existing distance metric learning approaches use fully labeled data to learn the sample similarities in an embedding space. We present a
                self-training framework, <i>SLADE</i>, to improve retrieval performance by leveraging additional unlabeled data. We first train a teacher model on the labeled
                data and use it to generate pseudo labels for the unlabeled data. We then train a student model on both labels and pseudo labels to generate final feature embeddings.
                We use self-supervised representation learning to initialize the teacher model. To better deal with noisy pseudo labels generated by the teacher network, we design a
                new feature basis learning component for the student network, which learns basis functions of feature representations for unlabeled data. The learned basis vectors better
                measure the pairwise similarity and are used to select high-confident samples for training the student network. We evaluate our method on standard retrieval
                benchmarks: CUB-200, Cars-196 and In-shop. Experimental results demonstrate that our approach significantly improves the performance over the state-of-the-art methods.</div>

            <div class="puba">Jiali Duan, Yen-Liang Lin, Son Tran, Larry Davis and C.-C. Jay Kuo</div>
            <div class="pubv">Preprint</div>
            <div class="publ">
              <ul>
                <li><a href="https://arxiv.org/abs/2011.10269">arXiv</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--CVPR2021-->


       <!--RL-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="papers/curri/curri-rl-cover.png" style="height:500; width:auto;">
            <img src="papers/curri/curri-rl-env.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Challenge Reinforcement Learning with Interactive Curriculum</b></div>
            <div class="pubd">Curriculum reinforcement learning benefits training by focusing on examples of gradual increasing difficulty
                that are neither too hard nor too easy. However, it's not always intuitive to define a curriculum that lies within this ``range of interest''.
                On one end of spectrum, a SOTA algorithm that learns from scratch may fail to collect any reinforcing signal. While on the other end,
                an automatic task-agnostic curriculum could ``overfit'' in an early phase that prevents it from further adaptation. In contrast,
                human has an innate ability to improvise and adapt when confronted with different scenarios, which we utilize to provide
                explainability and guidance for curriculum reinforcement learning. We first identify the ``inertial'' problem in automatic curriculum
                and then propose a simple interactive curriculum framework that works in environments that require millions of interactions.</div>

            <div class="puba">Jiali Duan, Yilei Zeng, Yang Li, Emilio Ferrara, Lerrel Pinto, C.-C. Jay Kuo, Stefanos Nikolaidis</div>
            <div class="pubv">Preprint</div>
            <div class="publ">
              <ul>
                <li><a href="papers/curri/curriculum-rl.pdf">PDF</a></li>
                <li><a href="https://github.com/davidsonic/interactive-curriculum-reinforcement-learning">Demo (Github)</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--RL-->

      <!--SCMLS-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="papers/scmls/tsne.png" style="height:500; width:auto;">
            <!--<img src="papers/scmls/compat.png" style="height:500; width:auto;">-->
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Fashion Compatibility Recommendation via Unsupervised Metric Graph Learning</b></div>
            <div class="pubd">In the task of fashion compatibility prediction, the goal is to pick an item from a candidate list to complement a partial
                outfit in the most appealing manner. Existing fashion compatibility recommendation work comprehends clothing images in a single metric space and lacks
                detailed understanding of users’ preferences in different contexts. To address this problem, we propose a novel Metric-Aware Explainable Graph Network (MAEG).
                In MAEG, we propose an unsupervised approach to obtain representation of items in a metric-aware latent semantic space. Then, we develop a graph filtering network and Pairwise Preference
                ttention module to model the interactions between users’ preferences and contextual information. Experiments on real world dataset reveals that MAEG not only outperforms
                the state-of-the-art methods, but also provides interpretable insights by highlighting the role of semantic attributes and contextual relationships among items.</div>

            <div class="puba">Jiali Duan, Xiaoyuan Guo, Son Tran and C.-C. Jay Kuo</div>
              <div class="pubv">SCMLS 2020</a></div>
            <div class="publ">
              <ul>
                <li><a href="papers/fashion-graph.pdf">Preprint</a></li>
                <li><a href="papers/scmls/scmls20_paper_27.pdf">SCMLS</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--SCMLS-->

          <!--WACV PortraitGAN-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="wacv/framework2.png" style="height:500; width:auto;">
            <img src="wacv/interact.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>PortraitGAN for Flexible Portrait Manipulation</b></div>
            <div class="pubd">Previous methods have dealt with discrete manipulation of facial attributes such as smile, sad, angry, surprise etc, out of
                canonical expressions and they are not scalable, operating in single modality. In this paper, we propose a novel framework that supports
                continuous edits and multi-modality portrait manipulation using adversarial learning. Specifically, we adapt cycle-consistency
                into the conditional setting by leveraging additional facial landmarks information. This has two effects: first cycle mapping induces
                bidirectional manipulation and identity preserving; second pairing samples from different modalities can thus be utilized. To ensure
                high-quality synthesis, we adopt texture-loss that enforces texture consistency and multi-level adversarial supervision that facilitates
                gradient flow. Quantitative and qualitative experiments show the effectiveness of our framework in performing flexible and multi-modality
                portrait manipulation with photo-realistic effects. </div>

            <div class="puba">Jiali Duan, Xiaoyuan Guo, and C.C-Jay Kuo</div>
            <div class="pubv">APSIPA 2020</div>
            <div class="publ">
              <ul>
                <li><a href="papers/PortraitGAN-final.pdf">PDF</a></li>
                <li><a href="https://github.com/davidsonic/Flexible-Portrait-Manipulation">Code (Coming Soon)</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--PortraitGAN-->


      <!--IROS-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="scr/iros.png" style="height:500; width:auto;">
            <img src="scr/grasp_demo1.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Robot Learning via Human Adversarial Games</b></div>
            <div class="pubd">Much work in robotics has focused on “human-in-the-loop” learning techniques that improve the efficiency of
the learning process. However, these algorithms have made the
strong assumption of a cooperating human supervisor that assists
the robot. In reality, human observers tend to also act in an
adversarial manner towards deployed robotic systems. We show
that this can in fact improve the robustness of the learned models
by proposing a physical framework that leverages perturbations
applied by a human adversary, guiding the robot towards more
robust models. In a manipulation task, we show that grasping
success improves significantly when the robot trains with a
human adversary as compared to training in a self-supervised
manner.  </div>

            <div class="puba">Jiali Duan, Qian Wang, Lerrel Pinto, C.-C. Jay Kuo, and Stefanos Nikolaidis</div>
              <div class="pubv">IROS 2019 (Best Paper Finalist), <a href="https://viterbischool.usc.edu/news/2019/11/showing-robots-tough-love-helps-them-succeed-finds-new-usc-study/">USC Media</a></div>
            <div class="publ">
              <ul>
                <li><a href="https://arxiv.org/abs/1903.00636">PDF</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--IROS-->

      <!--SCR-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="scr/iros2019-1.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Robust Grasping via Human Adversary</b></div>
            <div class="pubd">An interactive interface has been developed based on customized Mujoco, where human perturbation is applied to guide the learning of
            robust grasping behavior. We compared human adversary with simulated adversary and grasping policy learned in a self-supervised manner. Both quantitative and subjective user studies verify
            the effectiveness of human domain knowledge involved. We're the first work to study the effect of human-adversary in robotic learning.</div>

            <div class="puba">Jiali Duan, Qian Wang, Lerrel Pinto, C.-C. Jay Kuo, and Stefanos Nikolaidis</div>
            <div class="pubv">SCR 2019 (Spotlight)</div>
            <div class="publ">
              <ul>
                <li><a href="https://davidsonic.github.io/summary/caltech_symposium.pdf">PDF</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--SCR-->

       <!--Interpret-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="saak/Saak-pic1.png" style="height:500; width:auto;">
            <img src="saak/Saak-pic2.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Interpretable Convolutional Neural Networks via Feedforward Design</b></div>
            <div class="pubd">The model parameters of convolutional neural networks (CNNs) are determined by backpropagation (BP). In this work, we propose an interpretable
feedforward (FF) design without any BP as a reference. The FF design adopts a data-centric approach. It derives network parameters of the current layer based on data statistics from the output of the previous layer in
a one-pass manner. To construct convolutional layers, we develop a new signal transform, called the Saab (Subspace approximation with adjusted bias)
transform. It is a variant of the principal component analysis (PCA) with an added bias vector to annihilate activation’s nonlinearity. Multiple Saab transforms in cascade yield multiple convolutional layers. As to fully-connected
(FC) layers, we construct them using a cascade of multi-stage linear least squared regressors (LSRs). The classification and robustness (against adversarial attacks) performances of BP- and FF-designed CNNs applied to the
MNIST and the CIFAR-10 datasets are compared. Finally, we comment on the relationship between BP and FF designs.</div>

            <div class="puba">C.-C. Jay Kuo, Min Zhang, Siyang Li, Jiali Duan and Yueru Chen</div>
            <div class="pubv">IJCV 2019</div>
            <div class="publ">
              <ul>
                <li><a href="https://arxiv.org/pdf/1810.02786.pdf">PDF</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--Interpret-->

       <!--Yzhu-->
      <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">

	    <br>
            <img src="saak/yzhu1.png" style="height:500; width:auto;">
	    <br><br>
          </div>
        </div>  <!--col-md-6-->

        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>An Interpretable Generative Model for Handwritten Digit Image Synthesis</b></div>
            <div class="pubd">Modern image generative models, such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs),
are trained by backpropagation (BP). The training process is complex and the underlying mechanism is difficult to explain.
                We propose an interpretable generative model for handwritten digits synthesis is proposed in this work.</div>
            <div class="puba">Yao Zhu, Saksham Suri, Pranav Kulkarni, Yueru Chen, Jiali Duan and C.-C. Jay Kuo</div>
            <div class="pubv">ICIP 2019</div>
            <div class="publ">
              <ul>
                <li><a href="https://arxiv.org/pdf/1811.04507.pdf">PDF</a></li>
              </ul>
            </div>

          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->
      <!--Yzhu-->

  	
  <!--ACM-TOMM 2017-->
  
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            
	    <br>
            <img src="ACM-TOMM/acm-tomm-gesture.png" style="height:500; width:auto;"> 
	    <br><br>
          </div>  
        </div>  <!--col-md-6-->
      
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>A Unified Framework for Multi-Modal Isolated Gesture Recognition</b></div>
            <div class="pubd">In this paper, we focus on isolated gesture recognition and explore different modalities by involving
RGB stream, depth stream and saliency stream for inspection. Our goal is to push the boundary of this
realm even further by proposing a unified framework which exploits the advantages of multi-modality
fusion. Specifically, a spatial-temporal network architecture based on consensus-voting has been pro-
posed to explicitly model the long term structure of the video sequence and to reduce estimation vari-
ance when confronted with comprehensive inter-class variations. In addition, a 3D depth-saliency con-
volutional network is aggregated in parallel to capture subtle motion characteristics.</div>

            <div class="puba">Jiali Duan, Jun Wan, Shuai Zhou, Xiaoyuan Guo, and Stan Z. Li</div>
            <div class="pubv">ACM-TOMM 2017 (Accepted) </div>
            <div class="publ">
              <ul>
                <li><a href="ACM-TOMM/ACM-TOMM.pdf">PDF</a></li>
              </ul>
            </div>
            
          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->

  	<!--cvpr 2017-->
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            

            <img src="cvpr/cvpr-2SCVN.png" style="height:200px; width:auto;"> 
	    <br><br>
	    <img src="cvpr/cvpr-result-table.png" style="height:150px; width:auto;" >
          </div>  
        </div>  <!--col-md-6-->
      
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Multi-Modality Fusion based on Consensus-Voting and 3D Convolution for
Isolated Gesture Recognition</b></div>
            <div class="pubd"> We propose a convolutional two-stream consensus voting network (2SCVN) which explicitly models both the short-term and long-term structure of
the RGB sequences. To alleviate distractions from background, a 3d depth-saliency ConvNet stream (3DDSN) is
aggregated in parallel to identify subtle motion characteristics. These two components in an unified framework significantly improve the recognition accuracy. On the challenging Chalearn IsoGD benchmark, our proposed method
outperforms the first place on the leader-board by a large
margin (10.29%) while also achieving the best result on
RGBD-HuDaAct dataset (96.74%).</div>

            <div class="puba">Jiali Duan, Shuai Zhou, Jun Wan, Xiaoyuan Guo, and Stan Z. Li</div>
            <div class="pubv">Arxiv</div>
            <div class="publ">
              <ul>
                <li><a href="https://arxiv.org/pdf/1611.06689v1.pdf">PDF</a></li>
              </ul>
            </div>
            
          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->


  	<!--ccbr 2016-->
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            

            <img src="ccbr/results.png" style="height:200px; width:auto;"> 
            <img src="ccbr/enlarged.png" style="height:200px; width:auto;" >	
	    <br><br>
	    <img src="ccbr/table1.png" style="height:100px; width:auto;" >
          </div>  
        </div>  <!--col-md-6-->
      
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Face Classification: A Specialized Benchmark Study</b></div>
            <div class="pubd"> We conduct a specialized
benchmark study in this paper, which focuses on face classifica
tion. We start with face proposals, and build a benchmark dataset with
about 3.5 million patches for two-class face/non-face classification. Results with several baseline algorithms show that, without the help of
post-processing, the performance of face classification itself is still not
very satisfactory, even with a powerful CNN method. We’ll release this
benchmark to help assess performance of face classification only, and ease
the participation of other related researchers.</div>

            <div class="puba">Jiali Duan, Shengcai Liao, Shuai Zhou, and Stan Z. Li</div>
            <div class="pubv">CCBR 2016 (Best Student Paper) </div>
            <div class="publ">
              <ul>
                <li><a href="https://davidsonic.github.io/index/ccbr_2016.html">Project</a></li>
                <li><a href="ccbr/ccbr2016.pdf">PDF</a></li>
		        <li><a href="https://github.com/davidsonic/face_classification_ccbr2016">Code (Github)</a></li>
                <li><a href="ccbr/FCB-CCBR2016.pptx">Presentation</a></li>
              </ul>
            </div>
            
          </div>  <!--pub-->
     </div>  <!--col-md-6-->
    </div> <!--row-->
   </div> <!--pubwrap-->


<!--ACCV Workshop-->
    <div class="pubwrap">
      <div class="row">
        <div class="col-md-6">
          <div class="pubimg">
            

            <img src="accv/AFW_NEW_SMALL.png" style="height:200px; ">
            <img src="accv/FDDB_NEW_SMALL.png" style="height:200px;">
	    
          </div>  
        </div>  <!--col-md-6-->
      
        <div class="col-md-6">
          <div class="pub">
            <div class="pubt"><b>Face Detection by Aggregating Visible Components</b></div>
            <div class="pubd">In this paper, we propose a novel face detection method called
Aggregating Visible Components (AVC), which addresses pose variations
and occlusions simultaneously in a single framework with low complexi-
ty. The main contributions of this paper are: (1) By aggregating visible
components which have inherent advantages in occasions of occlusions,
the proposed method achieves state-of-the-art performance using only
hand-crafted feature; (2) Mapped from meanshape through component-
invariant mapping, the proposed component detector is more robust to
pose-variations (3) A local to global aggregation strategy that involves
region competition helps alleviate false alarms while enhancing localiza-
tion accuracy.	</div>

            <div class="puba">Jiali Duan, Shengcai Liao, Xiaoyuan Guo, and Stan Z. Li</div>
            <div class="pubv">ACCV Workshop 2016 (Oral)</div>
            <div class="publ">
              <ul>
                <li><a href="accv_2016.html">Project</a></li>
                <li><a href="accv/accv2016finalpaper.pdf">PDF</a></li>
                <li><a href="accv/WFI-ACCV2016.pptx">Presentation (+audio)</a>
              </ul>
            </div>
            
          </div>  <!--pub-->
        </div> <!--col-md-6--> 
	<br><br>
        <div style="text-align:center;">
	<img src="accv/pipeline.png" style="width:80%;"> 	
 	</div>
      </div>  <!--row-->
        
    </div>  <!--pubwrap-->
      

  </div> <!--pubs-->
</div>  <!--container-->

<div class="container">

    <h2>Competition</h2>
    <div class="ctr">
       <div class="hht">
    	  Autumn 5th/November/2014: I was awarded second-place for <br>
          <a href="http://news1.ucas.ac.cn/Home/Detail18/16df15f8-cc21-4350-8c9f-e54ace93a0ce"> English Speaking Competition held by University of Chinese Academy of Sciences </a>
     	
       </div>
       <a href="images/speech-ucas.png"> 
       <img src="images/ucas-speaking.jpg" style="width:60%"></a>
        <div>
	 	<audio controls>
	  	  <source src="speech.mp3" type="audio/mpeg">
		</audio>
	</div>
    </div>

    <div class="ctr">
	<div class="hht">
	 Winter December/2012: Shanghai Final on behalf of ECUST for <br>
         <a href="">
            21st Century Coca-Cola Cup National English Speaking Competition</a> 
	</div>
	       <a href="images/outstanding.jpg"> 
	       <img src="images/shanghai-english-pic.jpg" style="width:60%"></a>
    </div>

    <div class="ctr">
	<div class="hht">
	 2012/2013 I was awarded with Second prize and honorable mention for <br>
         <a href="images/math-china.jpg">
            National Mathematical Contest in Modeling</a> and
	 <a href="images/math-US.png">MCM/ICM</a> respectively
	</div>
    </div>

    <!--<div class="row">-->
	<!--<div class="col-md-6">-->
 	   <!--<div style="font-size:25px;">Academic</div>-->

		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/beijing-english.png">Honarable Mention in Beijing English Speaking Competition for Master Student&ndash;&gt;-->
		     <!--<a href="">Honarable Mention in Beijing English Speaking Competition for Master Student-->
		     <!--</a>-->
		   <!--</div>-->

		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/shanghai-tech.jpg">Participate in Summer Camp for Recommended Student for Admission of ShanghaiTech University</a>&ndash;&gt;-->
		     <!--<a href="">Participate in Summer Camp for Recommended Student for Admission of ShanghaiTech University</a>-->
		   <!--</div>-->

   		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/math-US.png">Honarable Mention for MCM/ICM Mathematical Modeling Contest for American Students</a>&ndash;&gt;-->
		     <!--<a href="">Honarable Mention for MCM/ICM Mathematical Modeling Contest for American Students</a>-->
		   <!--</div>-->

	 	   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/math-china.jpg">Second Prize for National Mathematical Contest in Modeling&ndash;&gt;-->
		     <!--<a href="">Second Prize for National Mathematical Contest in Modeling-->
		     <!--</a>-->
		   <!--</div>-->
		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/math-shanghai.jpg">First Prize for Shanghai Mathematical Contest in Modeling&ndash;&gt;-->
		     <!--<a href="">First Prize for Shanghai Mathematical Contest in Modeling-->
		     <!--</a>-->
		   <!--</div>-->

	       <!---->

	<!--</div>-->

	<!--<div class="col-md-6">-->
 	   <!--<div style="font-size:25px;">Certificates</div>-->
		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/Advanced-interpretation.jpg">Adavanced-Level English Interpretation Accreditation Examination Certificate&ndash;&gt;-->
		     <!--<a href="">Adavanced-Level English Interpretation Accreditation Examination Certificate-->
		     <!--</a>-->
		   <!--</div>-->


		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/exe-student1.png">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2014-2015&ndash;&gt;-->
		     <!--<a href="">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2014-2015-->
		     <!--</a>-->
		   <!--</div>-->

		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/exe-student2.png">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2015-2016&ndash;&gt;-->
		     <!--<a href="">Awarded with Excellent Student in University of Chinese Academy of Sciences for 2015-2016-->
		     <!--</a>-->
		   <!--</div>-->
		   <!---->

		   <!--<div class="ts">-->
		     <!--&lt;!&ndash;<a href="images/volunteer.png">Volunteer and Referee for the 16th China Adolescent Robotics Competition&ndash;&gt;-->
		     <!--<a href="">Volunteer and Referee for the 16th China Adolescent Robotics Competition-->
		     <!--</a>-->
		   <!--</div>-->
	   <!---->

	<!--</div>-->
    <!--</div>  &lt;!&ndash;row&ndash;&gt;-->

    <!--<div class="showmore" id="showmorepubs">-->
       <!--show more-->
       <!--</div>-->

    <!--<div id="morepubs">-->
    	<!--<div class="row">-->
    		<!--<div class="col-md-6">-->

    		   <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/2011-runner-up.jpg">Second Prize for 2011 English Debating Competition in ECUST&ndash;&gt;-->
			     <!--<a href="">Second Prize for 2011 English Debating Competition in ECUST-->
			     <!--</a>-->
			   <!--</div>-->

	           <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/uchallenge-2nd.jpg">Second Prize for 2011 Uchallenge English Speaking Competition&ndash;&gt;-->
			     <!--<a href="">Second Prize for 2011 Uchallenge English Speaking Competition-->
			     <!--</a>-->
			   <!--</div>-->

	       	   <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/ECUST-1st.jpg">First Prize for 2011 Extemporaneous English Speaking Competition in ECUST&ndash;&gt;-->
			     <!--<a href="">First Prize for 2011 Extemporaneous English Speaking Competition in ECUST-->
			     <!--</a>-->
			   <!--</div>-->

		   	   <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/electronic-comp.jpg">Second Prize for <i> Electronic Assembly-Debugging and Developing Competition</i> in ECUST&ndash;&gt;-->
			     <!--<a href="">Second Prize for <i> Electronic Assembly-Debugging and Developing Competition</i> in ECUST-->
			     <!--</a>-->
			   <!--</div>-->

			   <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/new-ecust.jpg">First Prize for <i> English Speaking Competition for New Students</i> in ECUST&ndash;&gt;-->
			     <!--<a href="">First Prize for <i> English Speaking Competition for New Students</i> in ECUST-->
			     <!--</a>-->
			   <!--</div>-->

			   <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/scholarship-ecust.jpg">Scholarships Awarded in ECUST&ndash;&gt;-->
			     <!--<a href="">Scholarships Awarded in ECUST-->
			     <!--</a>-->
			   <!--</div>-->

    		<!--</div>-->

    		<!--<div class="col-md-6">-->

	    		<!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/mid-level.jpg">Mid-Level English Interpretation Accreditation Examination Certificate&ndash;&gt;-->
			     <!--<a href="">Mid-Level English Interpretation Accreditation Examination Certificate-->
			     <!--</a>-->
			   	<!--</div>-->

    			<!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/FPGA.jpg">FPGA Embedded Application Programmer Certificate&ndash;&gt;-->
			     <!--<a href="">FPGA Embedded Application Programmer Certificate-->
			     <!--</a>-->
			    <!--</div>-->

			    <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/photo-ucas.png">Third Prize for <i> The Most Beautiful Hometown </i> photography competition&ndash;&gt;-->
			     <!--<a href="">Third Prize for <i> The Most Beautiful Hometown </i> photography competition-->
			     <!--</a>-->
			   <!--</div>-->

			   <!--<div class="ts">-->
			     <!--&lt;!&ndash;<a href="images/poem-ucas.png">Honarable Mention for <i> The Most Beautiful Hometown </i> poem competition&ndash;&gt;-->
			     <!--<a href="">Honarable Mention for <i> The Most Beautiful Hometown </i> poem competition-->
			     <!--</a>-->
			   <!--</div>-->

    		<!--</div>-->

    	<!--</div>-->
    <!--</div>-->

</div>


<hr class="soft">
<!--pet project
<div class="container">
   <h2>Pet Project</h2>

   <div class="row">
       <div class="col-md-4"> 
           <div class="pp">
              <a href="http://159.226.21.92:5901">
                <img src="images/django-blog.png" class="imgb">
              </a>
           </div>
           <div class="ppt">Django+mod_wsgi+apache</div>
           <div class="ppd">Django is a flexible and powerful tool for building 
           personal web-site in that it has very good extensibility and is easy to adminster the server and the database. Mod_wsgi and apache2 have been used to deploy the website, feel free to <a href="http://159.226.21.92:5901"> login </a>and comment!</div>
       </div>

       <div class="col-md-4"> 
           <div class="pp">
              <a href="http://159.226.21.92/multi_task/">
                <img src="images/multi-task.png" class="imgb">
              </a>
           </div>
           <div class="ppt">Multi-task Face Attributes</div>
           <div class="ppd">Multi-task proves to be an effective way for enhancing the performance of Convolutional Neural Network. This time, it is used to predict the age, gender and nationality. Experiment shows that multi-task prediciton achieves higher accuracy compared to single task alone. Single task has also been posted to predict age, gender respectively. You can upload you image to take a try. Have fun!</div>
       </div>

       <div class="col-md-4"> 
           <div class="pp">
              <a href="http://159.226.21.92/smile/">
                <img src="images/smile.png" class="imgb">
              </a>
           </div>
           <div class="ppt">Smiling Prediction</div>
           <div class="ppd">Smiling prediction can be reformulated as a prediction task. Given an facial-image of a person, a CNN network will automatically learns to predict to what extent that person is smiling. The website is deployed on the server using flask+tornado+nginx. Try it yourself and see if it's accurate!</div>
       </div>

  </div>  
  <br><br>


  <div>
     <div class="row">

       <div class="col-md-4"> 
           <div class="pp">
              <a href="http://159.226.21.92/gender">
                <img src="images/gender.png" class="imgb">
              </a>
           </div>
           <div class="ppt">Gender Classification</div>
           <div class="ppd">It turns out that computer often gets confused about the sextuality of a person. Illumination changes is one thing that has to be taken good care of.</div>
       </div>

       <div class="col-md-4"> 
           <div class="pp">
              <a href="http://159.226.21.92/age">
                <img src="images/age.png" class="imgb">
              </a>
           </div>
           <div class="ppt">Age Classification</div>
           <div class="ppd">Same person with different appearnces would engender different age predictions. Sounds strange? This is exactly what's challenging researchers of this area. Generally, we want the classifier to be robust to pose-variations, illuminance variance and even makeups. CNN is exciting in that it learns the most features for itself without human interference!</div>
       </div>

       <div class="col-md-4"> 
           <div class="pp">
              <a href="">
                <img src="images/spider.png" class="imgb">
              </a>
           </div>
           <div class="ppt">Scrapy the imdb</div>
           <div class="ppd">To get sufficient data for CNN training, I have made an attempt to scrap the imdb using Scrapy. I've never used Regular Expression so frequently before! The reference image of person is first acquired and then his/her associated pics. One tricky part is to use Selenium to simulate JS for generating requests.</div>
       </div>

     </div> 
  </div> 
 
</div>  -->

<hr>
  <div id="misc">
    <div>Contact Information</div>
    <div class="umisc">Want to know more about my project and research experience? Feel free to contact me at any time!</div>
    <div class="umisc">Email: jli.duan@gmail.com</div>
  </div>

  <!--place js at the end-->
  <script src="js/bootstrap.min.js"></script>
  <script>

  //
  var more_projects_shown = false;
$(document).ready(function() {
  $("#showmoreprojects").click(function() {
    if(!more_projects_shown) {
      $("#moreprojects").slideDown('fast', function() {
        $("#showmoreprojects").text('hide');
      });
      more_projects_shown = true;
    } else {
      $("#moreprojects").slideUp('fast', function() {
        $("#showmoreprojects").text('show more');
      });
      more_projects_shown = false;
    }
  });

  var more_pubs_shown = false;
  $("#showmorepubs").click(function() {
    if(!more_pubs_shown) {
      $("#morepubs").slideDown('fast', function() {
        $("#showmorepubs").text('hide');
      });
      more_pubs_shown = true;
    } else {
      $("#morepubs").slideUp('fast', function() {
        $("#showmorepubs").text('show more');
      });
      more_pubs_shown = false;
    }
  });

});

  //
  </script>
  </body>
</html>
